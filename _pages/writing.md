---
permalink: /writing/
title: "Short Form and Ideas"
excerpt: "Nathaniel's short form writing"
author_profile: true
redirect_from: 
  - /shortform/
  - /writing.html
  - /shortform.html
---
This is a place for some ideas I've had which seem worth keeping track of. Some of them may be turned into longer essays at some point. A few were originally written as part of Serimats applications, or alignment 101/201 exercises, or similar things. 

What LLMs know vs what they can produce
------
I want to distinguish between two categories that seem a bit confused to me: what an LLM knows vs what an LLM can produce. By saying an LLM “can produce” something, I mean there is some not-excessively-convoluted prompt which will cause an LLM to respond with the thing. For example, “Who was the first president of the USA?” will cause chatGPT to respond with (something involving) “George Washington”, so I will say it can produce that information. But, as we’ve seen from many hallucinatory examples, it can produce many things which have no correlation with the real world. For example, it is a theorem in mathematics that every simple group can be generated by two elements (don’t worry about what this means). But when asked about this, it will confidently respond:

 “No, not every simple group can be generated by two elements. In fact, it is known that there exist finite simple groups that cannot be generated by two elements. The smallest example of such a group is the alternating group $A_5$. This group is simple and has order 60, but it cannot be generated by fewer than three elements. More generally, a result known as the Schreier conjecture (proved by Pyber and Szabo in 1993) states that for any finite simple group $G$, there is a constant $c(G)$ such that $G$ cannot be generated by fewer than $c(G)$ elements. The value of $c(G)$ is not known for all finite simple groups, but it is known to be 2 for only a small number of exceptional cases. Therefore, while many finite simple groups can be generated by two elements, it is not true that every simple group can be generated in this way.”

When I responded “I’m sure every finite simple group can be generated by 2 elements.”
It came back with “I apologize for my previous response. You are correct that every finite simple group can be generated by two elements. This is a well-known result in group theory known as the "theorem of the two generators". The theorem of the two generators states that every finite non-abelian simple group can be generated by any two of its elements. This theorem was first proven by Walter Feit and John G. Thompson in their celebrated paper "Solvability of Groups of Odd Order" in 1963, and it is considered one of the most significant results in the theory of finite groups.”

(While Feit-thompson have a real paper from that year, by that name, the result I’m referencing was not completed until well after 1963. Also, googling “theorem of the two generators” turns up no results).

Thus, I would say this LLM can’t *produce* accurate information about this math question.

But what about what the LLM knows?

I would say an LLM *knows* a fact about the world only if there is a representation of the world inside the LLM, which contains that fact. Currently, I don’t know of any evidence that any LLM knows anything–indeed, I think there is some (limited) evidence that they do not. But I’m convinced by things like kenneth li’s othello work that a transformer-based architecture can, indeed, spontaneously generate a representation of the world. And it’s certainly possible that, if we were much better at interpretability work, we would be able to say with confidence that GPT does know, in this sense, that (eg) Washington was the first president.

Why does this matter?
Well, understanding what an LLM knows, in this sense, arguably gives a solution to Paul Christiano’s Eliciting Latent Knowledge (ELK) problem. If we could identify causal pathways between “the LLM knows x” and “the LLM can produce x”, I would take that as some evidence that the LLM is “being honest” with us.

I think there’s even more there, though. Consider the four possibilities for a piece of information, x. An LLM doesn’t know, and can’t produce, x is a boring case. An LLM can produce, but doesn’t know, x is the case I think we are mostly in now. An LLM can produce, and does know x is the case we wish we were assured of (along with some causality between the two–a situation where the LLM knows x, but that is not the reason for producing x is pretty easy to imagine). But what about a situation where the LLM knows x, but cannot produce x? I think this is a very interesting possibility. To see why, consider a hypothetical universe, with roughly our level (or perhaps a bit more) of ML/AI capabilities, but with less knowledge of physics–specifically, Newtonian physics still being the paradigm, with no special relativity (let alone general).

In such a universe, I can easily imagine hypothetical-GPT-5 getting a world representation that correctly understands relativity–there’s a pattern on various astronomy forums of people going “Have you looked at the data for last weeks (eclipse/transit of planet/hypothetical hubble photos)? It seems a little off, by (amount that Newtonian physics and relativity differ). Huh, weird.” and then dropping it. And HGPT-5 can predict those posts better if it captures the right understanding of physics in its world model. But any prompt of the form “explain everything about physics to me” will probably have HGPT-5 producing a standard explanation of newtonian physics–it knows relativity, but cannot produce it. 

This is one of many reasons I think interpretability work is important.


Shard theory and decay rates
------
I’m interested in decay rates of unused shards. A potential experiment to investigate this: what if the cheese starts out as “poison”, and then becomes edible after some medium number of time steps $t$? We train a maze-solving AI on small mazes in this regime–it will presumably learn to find the cheese, then wait nearby until the poison fades, then take the cheese. We then shift to a different training distribution of larger mazes, where the cheese is guaranteed to be inaccessible until after the poison has aged out and continue training (possibly two separate models, one with and one without weight decay). In the epochs where we train on the large mazes, we continue to evaluate on small mazes, with no poison. Does the AI maintain “cheese avoidance” before the medium number of time steps? How does this change with more training epochs in the large maze? With and without weight decay?

My assumption is that, with weight decay, we would see “cheese avoidance before t” steadily decrease with training epochs in the large mazes. I’d be very interested in seeing if the avoidance steadily decreased in terms of “how long the AI waits between arriving at the cheese and taking it” vs “chance the AI takes the cheese before t”. I’m also very interested in seeing whether or not this shard is maintained in the version without weight decay. My very-low-confidence guess is that it would vanish, but much more slowly than the version with weight decay.

I think this could be worthwhile for alignment inasmuch as we to what extent model behaviors which are learned early are maintained over time. In humans, if someone is a recovered heroin addict, there is some evidence that they continue to have the reward circuitry associated with their addiction for years afterwards, and are thus more vulnerable to relapse than someone who has never tried heroin. If we have an AI which was quite expensive to train, and we notice undesirable behaviors, then is it easy/possible for mild retraining/fine tuning to eliminate this entirely? Or has the initial training baked the undesirability into the deep structure of the net, and the only way to get something with no remnant of it is to re-initialize and retrain from scratch (possibly after having modified your training set to explicitly reinforce against the undesirability)?

Short term optimism, long term pessimism (relative to other alignment thinkers)
------
Near-term, it seems like a lot of alignment thinkers believe that GPT-7 or something will be…worth being scared of, as an agent. I disagree–My mental model of LLMs with self-supervised training regimes is fundamentally very unlikely to be world-destroying (except possibly as a tool, but I would count that as a problem with Putin/Xi/Kim Jong Un’s alignment, not with the AI’s). I don’t think LLMs have strong emergent desires or goals, and I think we have some evidence against the proposition that they even have coherent world-models. While I think increasing parameter count can certainly increase accuracy, Chinchilla convinced me that we are more limited by high-quality training data, and after using basically the entire internet, it’s not obvious where to go next. I think human intelligence is long-tailed, and while GPT-4 is arguably more intelligent than average people, I believe we are several architectural and training regime revolutions (on the scale of the “RNNs to transformers” revolution) away from AGI that can be consistently 95th percentile intelligence. I also think I have a higher opinion of academia and industry’s success at taking low-hanging fruit than many alignment researchers, so I am more skeptical of recursive self-improvement and extremely fast takeoff. I think before we have misaligned super-intelligent AI, we will have misaligned highly intelligent AI, which will cause problems big enough to be a serious wake-up call, but not to destroy all value.

Longer term, however, I think people are trying hard to make something much, much smarter than us and give it goals, and I think they will eventually be successful, albeit maybe not for another 50+ years. But I think we have no idea, and no framework that looks promising to me for getting an idea, as to how to ensure a system has the goal that we want--Alex Turner's “Reward is not the optimization target” was pretty instrumental in making me realize exactly how true that is. Moreover, I think I am deeply skeptical of humanity’s ability to coordinate on large problems, especially ones where some people think there is a strong benefit to defecting. So I see very little likelihood of much human value remaining in the universe around (say) 100 years from now. This makes me sad, and I really hope I’m wrong.

Confusion on values/goals vs behaviors 
------
Over the course of my time thinking about alignment, I've become increasingly confused about the idea of goals vs behaviors. This probably started with reading Scott Alexander's "Blue minimizing robot". I think this has begun to seem like a pretty fundamental question to me. The two feel very distinguishable for humans, but I haven't been able to formalize the difference (if any!) for AI. (In the RL setting, there is a formalism that distinguishes between them, but I really want an answer which is agnostic to how a system was created/trained/evolved). To make thing more concrete, GPT-4 very rarely says racist things (because that was a high priority during the RLHF phase of training). But is it fair to say it has a goal/value of not expressing racism? Or just that its behavior is typically not racist?

Compare with a person showering every day--that person has a behavior, but without more information, it's very difficult to tell if the behavior is due to (a) valuing being clean (b) valuing the feeling of showering (c) having a showering habit and not thinking much about it (d) other. In this human case, I can imagine hypotheticals that can distinguish the 3--if someone was given a hotel room with a shower with chlorinated water (fine for the feel of a shower, might not feel clean afterwards) and a (scifi) closet that cleaned them with nanobots in 5 seconds, then they probably use the closet and no shower if it's (a), and use the shower if it's (b) (and maybe also use the closet if (a&b)). Moreover, I think people have enough self-knowledge that you can just ask a (thoughtful intelligent) person "do you do x because you value it intrinsically, or in pursuit of another goal, or something else?" And get a good (although certainly not great!) answer, in the sense that it is predictive of actions if hypotheticals like the above actually happen. 

I don't know a general way to set up hypotheticals for gpt-4/arbitrary AI that seem informative to me on this axis. (Thanks to Ishita Dasgupta for some insightful conversations which helped me realize how confused I remain on this topic, and for pointing out [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3077926/) which is fascinating, and might deserve a post in its own right.)

In defense of interpretability
------
I've been reading some people recently who seem very skeptical of the usefulness of mechanistic interpretability for alignment. While I certainly don't think it is a complete solution, or even anything close to a majority, I do think it seems like a very good thing. In particular, it seems like the only way for us to "check our work" with whatever alignment solution we end up using. I don't expect it to be perfect, but to me it seems like the only candidate for this, and my estimation of our survival chances go monotonically up with our mechinterp knowledge.

Why alignment is hard in five words
------
We have no training data. We have literally no examples of the form "given these inputs, the correct action for a smarter-than-human AI to take is x". We have sentences describing the correct action, but that isn't the same. If we ask chat-GPT for what a batter should do when a pitcher throws a ball, it doesn't matter how good the resulting description of the home run is--if we actually take chat-GPT, it can't hit a baseball.

"Alignment is pre-paradigmatic"
------
One piece of advice I hear often directed to new researchers is the idea that it is important to form your own models of AI risk, or that people should try to come up with solutions to the problems before reading much from the alignment forum. I don't know how true I think this is, but if it is true, it seems like obviously very bad news. In a field where people know roughly what they are doing (eg, almost any academic field more than 50 years old) one of the most important things to do is come to appreciate the common views and assumptions shared by the field. I would argue this is true even if (and perhaps especially if) the ultimate goal is productive criticism of a field. If it is important to "form your own models," that suggests alignment thinkers have made no consensus progress in the field thus far.